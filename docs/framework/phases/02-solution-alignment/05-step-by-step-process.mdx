---
title: "The step-by-step process"
sidebar_label: "Step by Step"
description: "Five steps: divergence, structuring, convergence, external validation, and internal validation to achieve real consensus."
sidebar_position: 1
slug: /framework/phases/solution-alignment/step-by-step-process
tags: [phases, alignment, process, divergence, convergence, consensus]
---

# The step-by-step process

Solution Alignment has five steps. The first three (Divergence, Structuring, Convergence) are the intellectual work of finding and defining the solution. The last two (External validation, Internal validation) are the alignment work that turns a good idea into a shared mandate.

## Step 1. Divergence: generate without filtering

*Estimated duration: 1-2 sessions of 2-3 hours*

The first step is deliberately divergent. The goal is breadth, not quality. The team generates as many solutions to the defined problem as possible, without filtering, without judging, without worrying about viability.

This seems to contradict the rigor we demanded in Phase 1. It doesn't. Controlled divergence is a tool, not an abandonment of discipline. The Problem Statement acts as a frame: all solutions must respond to the defined problem, not to a problem imagined during the session.

### Divergence techniques

| Technique | How it works | When to use it |
|---|---|---|
| **How Might We** | The Problem Statement is reformulated as an open question: "How might we help managers anticipate stock-outs?" Each participant generates individual responses before sharing. | Always. It's the natural starting point for any divergence. |
| **Crazy 8s** | Each person draws 8 ideas in 8 minutes. Forces speed to overcome the internal filter that censors "bad" ideas. | When the team tends to converge too quickly or self-censor. |
| **Reverse brainstorming** | Instead of looking for solutions, the team looks for ways to make the problem worse. Then inverts each idea to generate solutions. | When the team is stuck or ideas are too conventional. |
| **Cross-industry analogies** | Solutions to similar problems in completely different industries are sought. "How does Uber solve the demand prediction problem?" | Design problems where lateral thinking is needed. |

### The Ideation Board

All generated ideas are captured visually on an Ideation Board. This artifact matters not for the ideas that are chosen, but for those that are discarded. The record of why each idea was discarded is organizational memory that prevents someone from proposing the same thing three months later without knowing it was already evaluated.

:::danger[Anti-pattern: The HiPPO Ideation]
HiPPO: Highest Paid Person's Opinion. The ideation session becomes one person's show. The stakeholder with the most authority proposes an idea in the first five minutes. The rest of the team spends the session elaborating variations of that idea instead of exploring real alternatives.

**How to prevent it:** Ideas are generated in silence and individually before sharing. Written on post-its, not spoken aloud. All are read before any are discussed. This way the VP's idea has the same visual size as the junior's.
:::

## Step 2. Structuring: decompose to understand

*Estimated duration: 1-2 sessions of 3-4 hours*

This is where thinking becomes rigorous. Ideas generated during divergence are organized, grouped, and subjected to decomposition across the four dimensions (business, model, architecture, data).

This step is the one most teams skip. They go directly from ideation to voting. The result: they choose solutions without understanding their real complexity, dependencies, or risks.

### The Solution Tree

For each candidate solution (typically 2-3 after initial grouping), the team builds a Solution Tree that decomposes the idea into its components by dimension:

```
Candidate solution: "AI-powered stock-out prediction"
│
├── Business
│   ├── Target KPI: reduce stock-outs from 15% to 3%
│   ├── Expected ROI: recover 12% of lost sales (~€X/year)
│   ├── Operating cost: tokens/inference × volume × frequency
│   └── Adoption model: who uses it and how does it change their workflow?
│
├── Model
│   ├── AI type: predictive model (not generative)
│   ├── Input: sales history + seasonality + external data
│   ├── Output: stock-out probability per SKU at 24-72h
│   ├── Required precision: >85% (below this, no trust is generated)
│   └── Acceptable error margin: false positives tolerable,
│       false negatives critical
│
├── Architecture
│   ├── Integration: ERP connector (daily batch → near real-time?)
│   ├── Deployment: cloud (latency not critical, scale yes)
│   ├── Interface: dashboard? push alerts? integration in existing system?
│   └── Security: sales data is not PII but is confidential
│
└── Data
    ├── Availability: 3-year history in ERP
    ├── Quality: peak season data reliable, off-season incomplete
    ├── Gaps: no unsatisfied demand data (lost sales)
    ├── Privacy: no personal data, compliance is not blocking
    └── Strategy: classic predictive model, possible enrichment with RAG
        for market trend context
```

:::info[Technique]
**The dimension completeness test:** After building the tree, the team reviews each dimension with a question: "If we completely ignore this branch, can the solution work?" If the answer is yes, the branch is not critical (but document it as an assumption). If the answer is no, it's an essential component that needs answers before moving forward.
:::

### The Assumptions Register

During structuring, assumptions inevitably emerge: things the team believes are true but hasn't verified. Each assumption is a risk. The work of this phase is not to verify all assumptions — that would be endless — but to classify them by risk and decide which are acceptable and which are blocking.

| Assumption type | Definition | What to do with it |
|---|---|---|
| **Verified** | We have evidence it's true. | Document the evidence and move forward. |
| **Low risk** | If false, the solution adjusts without major impact. | Document and accept as minor risk. |
| **High risk** | If false, the solution doesn't work or changes fundamentally. | Verify before moving to Step 3. Don't choose a solution whose viability depends on unverified high-risk assumptions. |
| **Unknown** | We don't know if it's true and don't know how to verify it. | Flag as open risk. If the solution depends on this assumption, it needs a contingency plan. |

:::info[Example: Inventory project assumptions]
**Verified:** "The ERP has 3 years of sales history accessible via API." → Confirmed by IT.

**Low risk:** "Store managers will use the tool if it integrates into their current workflow." → Probable based on interviews, but not certain. If false, training needs to be added.

**High risk:** "The ERP sales data is granular enough to predict at the individual SKU level." → If it isn't, the model doesn't work. **Verify before choosing this solution.**

**Unknown:** "Unsatisfied demand data (lost sales due to stock-outs) can be estimated from search patterns on the retailer's website." → Nobody has investigated this. If possible, it enormously enriches the model. If not, it works without it but with less precision.
:::

## Step 3. Convergence: decide with explicit criteria

*Estimated duration: 1 session of 2-3 hours*

Now the team has 2-3 structured candidate solutions, with their dimension trees and classified assumptions. The next step is not to vote. It's to evaluate against explicit criteria.

### The Decision Matrix

The Decision Matrix is a table where candidate solutions are evaluated against a set of weighted criteria. The criteria are not generic: they derive directly from the Problem Statement and from the dimension decomposition.

| Criterion | Weight | Source | Why it matters |
|---|---|---|---|
| **Impact on the defined problem** | 30% | Problem Statement → success criterion | How much does this solution move the needle on the KPI defined in the Problem Statement? A solution that solves 40% of the problem is not equal to one that solves 90%. |
| **Technical viability** | 20% | Architecture + Data dimensions | Can we build this with the data, infrastructure, and knowledge we have? How many high-risk assumptions does it depend on? |
| **Business viability** | 20% | Business dimension | Does the ROI justify the investment? Is the operating cost sustainable long-term? Is the adoption model realistic? |
| **Assumption risk** | 15% | Assumptions register | How many unverified high-risk assumptions does this solution have? How many unknowns? |
| **Adoption friction** | 15% | Business dimension + user feedback | How much does it change users' workflow? How much resistance to change is foreseeable? |

Each candidate solution receives a score of 1 to 5 on each criterion. The weighted score produces a ranking. But the ranking is not the decision. It's the input to the decision. If the highest-scoring solution has an unverified high-risk assumption that the second doesn't have, it may be more prudent to choose the second.

:::danger[Anti-pattern: The Decorative Matrix]
**What it is:** The team already knows which solution it wants. It builds the Decision Matrix and adjusts criterion weights until the result confirms the decision already made.

**How to detect it:** If you change the weights 10% in any direction and the result changes, the decision is fragile. If weights were "adjusted" after seeing initial results, it's manipulation disguised as method.

**How to prevent it:** Weights are defined and fixed before evaluating any solution. They are documented and not modified. If the result isn't what was expected, the response is to examine why, not to change the criteria.
:::

## Step 4. Client validation

*Estimated duration: 3-5 sessions of 30-45 minutes*

The chosen candidate solution is presented to real clients or users — preferably the same ones who validated the Problem Statement. The goal is not to ask them to evaluate it technically (that's not their job) but to confirm that the solution, as described, solves the problem they live with.

The key question is: "If this existed tomorrow, would anything change in your day-to-day?"

Responses usually fall into three categories:

- **"Yes, exactly."** → Validation. Move forward.
- **"Yes, but it's missing..."** → Iteration. Incorporate what's missing and present again.
- **"Hmm, it's not exactly what I need."** → Warning signal. Examine whether the team has drifted from the original problem or whether the solution doesn't translate the Problem Statement well.

Iterate until feedback confirms the solution solves the defined problem. This may take 2-3 rounds.

## Step 5. Internal stakeholder validation — real consensus

*Estimated duration: 2-4 sessions of 1-2 hours depending on complexity*

This is the most difficult step of the entire phase and the one most teams do poorly. Not because it's technically complex, but because it requires something organizations rarely practice: real consensus.

### What real consensus is

Consensus is not unanimity. It doesn't require everyone to be enthusiastic about the solution. It requires all relevant stakeholders to:

1. **Understand** what will be built and why.
2. **Accept** that the solution solves the defined problem.
3. **Know** the assumptions it contains and the associated risks.
4. **Commit** to supporting the solution when the first difficulties arise, not to abandoning it.

The fourth point is the most important and what distinguishes consensus from permission. A director who says "go ahead" in a meeting but withdraws support at the first setback never gave consensus. They gave permission. Permission is fragile. Consensus is resilient.

### What consensus is NOT

| What looks like consensus | Why it isn't | What it produces |
|---|---|---|
| **The boss says "go ahead"** | One person approved. The rest don't know or don't understand. | The team builds without real support. At the first problem, nobody defends the solution. |
| **Nobody objects in the meeting** | Silence is not agreement. It may be disinterest, fear of dissenting, or lack of understanding. | Objections that appear during construction, when it's too late. |
| **Majority votes in favor** | The minority that voted against still disagrees. Their objections didn't disappear. | Passive sabotage. "I already said this wasn't going to work." |
| **The document is approved** | Signing a document is not understanding it. Many stakeholders sign without reading. | Disagreements that emerge when they see the actual result and say "this isn't what I expected." |

:::info[Technique]
**The Corridor Consensus Test:** After the alignment session, find each stakeholder individually — in the hallway, at a coffee, on a quick call. Ask them: "If you had to explain to your team what we're going to build and why, what would you tell them?" If they can't explain it, or if their explanation is significantly different from another stakeholder's, there is no consensus. There is a signed document.
:::

### Managing dissent

Not all disagreements must be resolved. Some are legitimate and should be documented. What is not acceptable is invisible disagreement — the kind that exists but nobody names.

When a stakeholder has an objection the team cannot resolve, it's documented in the Feedback Log with three elements: what they say, why they say it, and what would be needed to resolve their objection. This record is not a concession. It's honesty. Solutions built on hidden disagreements fail in ways nobody can predict.
